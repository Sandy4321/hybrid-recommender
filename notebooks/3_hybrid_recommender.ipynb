{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Train Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from keras.layers import *\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from livelossplot import PlotLossesKeras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df: dataframe containing features to be encoded\n",
    "# columns: list of columns to be encoded\n",
    "def one_hot_encode(df, columns):\n",
    "    ohe = OneHotEncoder()\n",
    "    ohe_features = pd.DataFrame(ohe.fit_transform(df[columns]).toarray())\n",
    "    ohe_features.columns = ohe.get_feature_names()\n",
    "    df = pd.concat([df, ohe_features], axis=1)\n",
    "    df = df.drop(columns = categorical_features)\n",
    "    return df\n",
    "\n",
    "\n",
    "# df: dataframe containing features to be encoded\n",
    "# columns: list of columns to be encoded\n",
    "def label_encode(df, columns):\n",
    "    le = LabelEncoder()\n",
    "    df[columns] = df[columns].apply(le.fit_transform)\n",
    "    return df\n",
    "\n",
    "\n",
    "# df: dataframe containing text to be vectorized\n",
    "# column: string name of text column\n",
    "# vectorizer: scikit learn vectorizer - CountVectorizer or TfidfVectorizer\n",
    "def vectorize_text(df, column, vectorizer):\n",
    "    text = df[column].replace(np.nan, ' ').tolist()\n",
    "    X = vectorizer.fit_transform(text)\n",
    "    df[column+'_features'] = list(X.toarray())\n",
    "#     word_vecs = pd.DataFrame(X.toarray())\n",
    "    df.drop(columns=column, inplace=True)\n",
    "#     df = pd.concat([df, word_vecs], axis = 1)\n",
    "    return df\n",
    "\n",
    "\n",
    "# vectorizes columns that include a list that should be broken out into one-hot-encoded features\n",
    "# for example, a column containing lists like [\"red\", \"green\", \"blue\"] will be transformed into 3 columns with 0/1 indicators\n",
    "# df: dataframe containing column to be vectorized\n",
    "# column: column containing list of features\n",
    "def vectorize_columns(df, columns):\n",
    "    for column in columns:\n",
    "        df[column] = df[column].fillna('[]')\n",
    "        df[column] = df[column].apply(lambda x: x.strip('][').split(', '))\n",
    "        features = df[column].apply(frozenset).to_frame(name='features')\n",
    "        for feature in frozenset.union(*features.features):\n",
    "            new_col = feature.strip('\\'').lower()\n",
    "            df[new_col] = features.apply(lambda _: int(feature in _.features), axis=1)\n",
    "        df = df.drop(columns = [column])\n",
    "    return df\n",
    "\n",
    "\n",
    "# feature_columns: list of column names that contain single features values\n",
    "# embedding_columns: list of column names that contain vector embeddings (image or text embeddings)\n",
    "def create_metadata_df(df, feature_columns, embedding_columns):\n",
    "    features = df[feature_columns].reset_index(drop=True)\n",
    "    embeddings = pd.DataFrame()\n",
    "    for column in embedding_columns:\n",
    "        embeddings = pd.concat([embeddings, pd.DataFrame(np.vstack(df[column]))], axis=1)\n",
    "    result = pd.concat([features,embeddings],axis=1)\n",
    "    return result\n",
    "\n",
    "\n",
    "# recommender with only user-item ratings and no user-item features\n",
    "def create_basic_network(n_items, n_users, n_factors):\n",
    "    item_input = Input(shape=[1], name=\"Item-Input\")\n",
    "    item_embedding = Embedding(n_items, n_factors, name=\"Item-Embedding\")(item_input)\n",
    "    item_vec = Flatten(name=\"Flatten-Items\")(item_embedding)\n",
    "    \n",
    "    user_input = Input(shape=[1], name=\"User-Input\")\n",
    "    user_embedding = Embedding(n_users, n_factors, name=\"User-Embedding\")(user_input)\n",
    "    user_vec = Flatten(name=\"Flatten-Users\")(user_embedding)\n",
    "    \n",
    "    prod = Dot(name=\"Dot-Product\", axes=1)([item_vec, user_vec])\n",
    "    \n",
    "    model = Model([user_input, item_input], prod)\n",
    "    model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.001))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# user-item-ratings\n",
    "\n",
    "user_item_ratings_file = 'path to csv with schema: item_id, user_id, rating'\n",
    "\n",
    "ratings = pd.read_csv(user_item_ratings_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item features\n",
    "\n",
    "items_file = 'path to csv with schema: item_id, item_feature1, item_feature2, ..., item_featureN' \n",
    "\n",
    "items = pd.read_csv(items_file)\n",
    "items = items[['item_id','color','category','item_gender','description']]  # sample columns in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user features\n",
    "\n",
    "user_file = 'path to csv with schema: user_id, user_feature1, user_feature2, ..., user_featureN' \n",
    "users = users[['user_id','user_gender','colors','user_description']]  # sample columns in our dataset\n",
    "users = pd.read_csv(user_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item image encoded vectors\n",
    "\n",
    "images = pd.read_pickle('../data/image_vecs_encoded.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare item features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add image features to item data\n",
    "\n",
    "items = pd.merge(items,images,on='item_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode item categorical features from strings to ints\n",
    "item_cat_features = ['color', 'category', 'item_gender']  # TODO: replace with your categorical string features\n",
    "items = label_encode(items, item_cat_features)\n",
    "\n",
    "# vectorize item text descriptions\n",
    "tf_vectorizer = TfidfVectorizer()\n",
    "items = vectorize_text(items, 'description', tf_vectorizer) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare user features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode user categorical features\n",
    "user_cat_features = ['user_gender']  # TODO: replace with your categorical string features\n",
    "users = label_encode(users, user_cat_features)\n",
    "\n",
    "# vectorize user features - split lists into one hot encoded columns\n",
    "users = vectorize_columns(users, ['colors']) # sample column that contains lists in our dataset, e.g. ['blue', 'purple']\n",
    "\n",
    "# if there is text associated with the user, vectorize it here (like a user request, profile description, or other)\n",
    "users = vectorize_text(users, 'user_description', tf_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add all metadata to ratings df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.merge(ratings, items, on='item_id')\n",
    "ratings = pd.merge(ratings, users, on='user_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(ratings, test_size=0.15, random_state=42)\n",
    "n_users = len(ratings.user_id.unique())\n",
    "n_items = len(ratings.item_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_recommender_v1(n_item_features, n_user_features, embedding_size):\n",
    "\n",
    "    user_id_input = Input(shape=[1], name='user')\n",
    "    item_id_input = Input(shape=[1], name='item')\n",
    "    item_meta_input = Input(shape=[n_item_features], name='item_features')\n",
    "    user_meta_input = Input(shape=[n_user_features], name='user_features')\n",
    "\n",
    "    user_embedding = Embedding(output_dim=embedding_size, input_dim=n_users, name='user_embedding')(user_id_input)\n",
    "    item_embedding = Embedding(output_dim=embedding_size, input_dim=n_items, name='item_embedding')(item_id_input)\n",
    "    item_metadata = Dense(units=embedding_size, name='item_metadata')(item_meta_input)\n",
    "    user_metadata = Dense(units=embedding_size, name='user_metadata')(user_meta_input)\n",
    "\n",
    "    user_vec = Flatten()(user_embedding)\n",
    "    item_vec = Flatten()(item_embedding)\n",
    "    item_vec = Add()([item_vec, item_metadata])\n",
    "    user_vec = Add()([user_vec, user_metadata])\n",
    "\n",
    "    input_vec = Concatenate()([user_vec, item_vec])#, item_metadata, user_metadata])\n",
    "\n",
    "    x = Dense(128, activation='relu')(input_vec)\n",
    "    x = Dropout(0.5)(x)\n",
    "    y = Dense(1)(x)\n",
    "\n",
    "    model = Model(inputs=[user_id_input, item_id_input, item_meta_input, user_meta_input], outputs=y)\n",
    "    model.compile(loss='mse', optimizer=Adam(lr=0.001), metrics=['mae'])\n",
    "    return model\n",
    "\n",
    " \n",
    "def hybrid_recommender_v2(n_item_features, n_user_features, embedding_size):\n",
    "\n",
    "    # users\n",
    "    user_id_input   = Input(shape=[1], name='user')\n",
    "    user_meta_input = Input(shape=[n_user_features], name='user_features')\n",
    "\n",
    "    user_embedding  = Embedding(output_dim=embedding_size, input_dim=n_users, name='user_embedding')(user_id_input)\n",
    "    user_vec        = Flatten()(user_embedding)\n",
    "    user_vec        = Dropout(0.5)(user_vec)\n",
    "    user_metadata   = Dense(units=embedding_size, name='user_metadata')(user_meta_input)\n",
    "    \n",
    "    # items\n",
    "    item_id_input   = Input(shape=[1], name='item')\n",
    "    item_meta_input = Input(shape=[n_item_features], name='item_features')\n",
    "    item_img_input  = Input(shape=[embedding_size], name='item_image_features') # autoencoded image features\n",
    "\n",
    "    item_embedding  = Embedding(output_dim=embedding_size, input_dim=n_items, name='item_embedding')(item_id_input)\n",
    "    item_vec        = Flatten()(item_embedding)\n",
    "    item_vec        = Dropout(0.5)(item_vec)\n",
    "    item_metadata   = Dense(units=embedding_size, name='item_metadata')(item_meta_input)\n",
    "\n",
    "    # join features \n",
    "    item_vec        = Add()([item_vec, item_metadata, item_img_input])\n",
    "    user_vec        = Add()([user_vec, user_metadata])\n",
    "\n",
    "    input_vec       = Concatenate()([user_vec, item_vec])#, item_metadata, user_metadata])\n",
    "\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(128, activation='relu')(input_vec)\n",
    "    x = Dropout(0.5)(x)\n",
    "    y = Dense(1)(x)\n",
    "\n",
    "    model = Model(inputs=[user_id_input, item_id_input, item_meta_input, user_meta_input, item_img_input], outputs=y)\n",
    "    model.compile(loss='mse', optimizer=Adam(lr=0.001), metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No autoencoded features - raw VGG16 embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata cols\n",
    "item_feature_cols = ['color','category','item_gender'] # item feature columns that contain a single value\n",
    "item_embedding_cols = ['image_features','description_features'] # item feature columns that contain a list of embeddings - applicable to image or text embeddings\n",
    "user_feature_cols = ['user_gender','rose gold','white','black','gray','gold','red','orange','natural','blue light'] # gender plus additional one-hot-encoded features\n",
    "user_embedding_cols = ['user_description_features']\n",
    "\n",
    "# prepare train & test inputs\n",
    "train_item_metadata = create_metadata_df(train, item_feature_cols, item_embedding_cols)\n",
    "test_item_metadata = create_metadata_df(test, item_feature_cols, item_embedding_cols)\n",
    "\n",
    "train_user_metadata = create_metadata_df(train, user_feature_cols, user_embedding_cols)\n",
    "test_user_metadata = create_metadata_df(test, user_feature_cols, user_embedding_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# architecture v1\n",
    "n_item_features = 6534\n",
    "n_user_features = 2797\n",
    "embedding_size = 256\n",
    "\n",
    "model = hybrid_recommender_v1(n_item_features, n_user_features, embedding_size)\n",
    "\n",
    "history = model.fit([train.user_id, train.item_id, train_item_metadata, train_user_metadata]\n",
    "                    , train.rating\n",
    "                    , batch_size=32, epochs=50\n",
    "                    , validation_split=0.1\n",
    "                    , validation_data=([test.user_id, test.item_id, test_item_metadata, test_user_metadata], test.rating)\n",
    "                    , callbacks = [PlotLossesKeras()]\n",
    "                    , shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using autoencoded features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture v1\n",
    "item_embedding_cols = ['image_features_encoded','description_features']\n",
    "\n",
    "train_item_metadata = create_metadata_df(train, item_feature_cols, item_embedding_cols)\n",
    "test_item_metadata = create_metadata_df(test, item_feature_cols, item_embedding_cols)\n",
    "\n",
    "train_user_metadata = create_metadata_df(train, user_feature_cols, user_embedding_cols)\n",
    "test_user_metadata = create_metadata_df(test, user_feature_cols, user_embedding_cols)\n",
    "\n",
    "n_item_features = 2694\n",
    "\n",
    "model = hybrid_recommender_v1(n_item_features, n_user_features, embedding_size)\n",
    "history = model.fit([train.user_id, train.item_id, train_item_metadata, train_user_metadata]\n",
    "                    , train.rating\n",
    "                    , batch_size=32, epochs=100\n",
    "                    , validation_split=0.1\n",
    "                    , validation_data=([test.user_id, test.item_id, test_item_metadata, test_user_metadata], test.rating)\n",
    "                    , callbacks = [PlotLossesKeras()]\n",
    "                    , shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture v2\n",
    "n_item_features = 2438\n",
    "\n",
    "item_embedding_cols = ['description_features']\n",
    "\n",
    "train_item_metadata = create_metadata_df(train, item_feature_cols, item_embedding_cols)\n",
    "test_item_metadata = create_metadata_df(test, item_feature_cols, item_embedding_cols)\n",
    "\n",
    "train_user_metadata = create_metadata_df(train, user_feature_cols, user_embedding_cols)\n",
    "test_user_metadata = create_metadata_df(test, user_feature_cols, user_embedding_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = hybrid_recommender_v2(n_item_features, n_user_features, embedding_size)\n",
    "\n",
    "best = ModelCheckpoint('../models/recommender.h5',\n",
    "                        monitor='val_loss',\n",
    "                        verbose=0,\n",
    "                        save_best_only=True,\n",
    "                        mode='auto')\n",
    "\n",
    "history = model.fit([train.user_id, train.item_id, train_item_metadata, train_user_metadata, np.vstack(train.image_features_encoded)]\n",
    "                    , train.rating\n",
    "                    , batch_size=32, epochs=100\n",
    "                    , validation_split=0.2\n",
    "                    , validation_data=([test.user_id, test.item_id, test_item_metadata, test_user_metadata, np.vstack(test.image_features_encoded)], test.rating)\n",
    "                    , callbacks = [PlotLossesKeras(), best]\n",
    "                    , shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained model\n",
    "model = load_model('../models/recommender.h5') # model generated from v2 architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep model inputs\n",
    "item_ids = items.item_id # all item ids \n",
    "num_items = len(item_ids)\n",
    "\n",
    "user_idx = users.sample(1).user_id.index[0] # select random user index\n",
    "user_data = users.loc[[user_idx]] # get data for selected user\n",
    "\n",
    "user_id = user_data.user_id.values[0] # get user id for selected user\n",
    "user_ids = np.array([user_id for i in range(num_items)]) # array of user id repeated to match number of items\n",
    "\n",
    "item_embedding_cols = ['description_features']\n",
    "item_metadata = create_metadata_df(items, item_feature_cols, item_embedding_cols)\n",
    "user_metadata = create_metadata_df(user_data, user_feature_cols, user_embedding_cols)\n",
    "user_metadata = user_metadata.loc[user_metadata.index.repeat(num_items)] # repeat user features by number of items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions\n",
    "preds = model.predict([user_ids, item_ids, item_metadata, user_metadata, np.vstack(ratings.image_features_encoded)])\n",
    "preds = np.array([x[0] for x in preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort and get top N recommendations (indices of items recommended)\n",
    "num_recs = 10\n",
    "rec_ids = (-preds).argsort()[:num_recs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get recommended item ids and ratings from indices above\n",
    "recs = [(item_ids[x],preds[x]) for x in rec_ids] # list of tuples - (item id, predicted rating)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
